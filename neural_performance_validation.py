#!/usr/bin/env python3
"""
Neural Network Performance Validation
Comprehensive analysis of neural vs heuristic performance
"""

import json
import time
import requests
import statistics
from datetime import datetime
from pathlib import Path

class NeuralPerformanceValidator:
    def __init__(self, server_url="http://localhost:8888"):
        self.server_url = server_url
        self.results = {
            'before_neural': None,  # Will load from previous results
            'after_neural': None,   # Will test current performance
            'comparison': {},
            'validation_timestamp': datetime.now().isoformat()
        }
    
    def load_previous_results(self):
        """Load baseline performance results from before neural activation"""
        try:
            with open('performance_investigation_results.json', 'r') as f:
                data = json.load(f)
                self.results['before_neural'] = data
                print("âœ… Loaded baseline heuristic performance data")
                return True
        except FileNotFoundError:
            print("âŒ No baseline performance data found")
            return False
    
    def test_current_performance(self):
        """Test current neural network performance"""
        print("ðŸ§  Testing current neural network performance...")
        
        # Use the same scenarios as the original investigation
        scenarios = {
            "simple_scenario": {
                "game": {"id": "neural-validation", "ruleset": {"name": "standard"}, "timeout": 500},
                "turn": 1,
                "board": {
                    "width": 11, "height": 11,
                    "food": [{"x": 8, "y": 8}],
                    "snakes": [{
                        "id": "test-snake", "name": "Test Snake",
                        "body": [{"x": 5, "y": 5}, {"x": 5, "y": 4}],
                        "head": {"x": 5, "y": 5}, "health": 90, "length": 2
                    }], "hazards": []
                },
                "you": {
                    "id": "test-snake", "name": "Test Snake", 
                    "body": [{"x": 5, "y": 5}, {"x": 5, "y": 4}],
                    "head": {"x": 5, "y": 5}, "health": 90, "length": 2
                }
            },
            "complex_scenario": {
                "game": {"id": "complex-validation", "ruleset": {"name": "standard"}, "timeout": 500},
                "turn": 45,
                "board": {
                    "width": 11, "height": 11,
                    "food": [{"x": 1, "y": 9}, {"x": 10, "y": 1}],
                    "snakes": [
                        {
                            "id": "test-snake", "name": "Test Snake",
                            "body": [{"x": 3, "y": 7}, {"x": 3, "y": 6}, {"x": 3, "y": 5}],
                            "head": {"x": 3, "y": 7}, "health": 75, "length": 3
                        },
                        {
                            "id": "opponent", "name": "Opponent",
                            "body": [{"x": 8, "y": 3}, {"x": 8, "y": 2}],
                            "head": {"x": 8, "y": 3}, "health": 80, "length": 2
                        }
                    ], "hazards": []
                },
                "you": {
                    "id": "test-snake", "name": "Test Snake",
                    "body": [{"x": 3, "y": 7}, {"x": 3, "y": 6}, {"x": 3, "y": 5}],
                    "head": {"x": 3, "y": 7}, "health": 75, "length": 3
                }
            }
        }
        
        current_results = {}
        
        for scenario_name, scenario_data in scenarios.items():
            print(f"  ðŸŽ¯ Testing {scenario_name}...")
            response_times = []
            
            for i in range(20):  # More iterations for better accuracy
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.server_url}/move", 
                        json=scenario_data, 
                        timeout=10
                    )
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        response_time_ms = (end_time - start_time) * 1000
                        response_times.append(response_time_ms)
                    
                    time.sleep(0.01)  # Small delay between requests
                    
                except Exception as e:
                    print(f"    âš ï¸ Request {i+1} failed: {e}")
                    continue
            
            if response_times:
                current_results[scenario_name] = {
                    'response_times': response_times,
                    'average': statistics.mean(response_times),
                    'median': statistics.median(response_times),
                    'min': min(response_times),
                    'max': max(response_times),
                    'std_dev': statistics.stdev(response_times) if len(response_times) > 1 else 0,
                    'success_rate': len(response_times) / 20 * 100
                }
                
                avg = current_results[scenario_name]['average']
                print(f"    âœ… Average: {avg:.2f}ms ({len(response_times)}/20 successful)")
            else:
                print(f"    âŒ All requests failed for {scenario_name}")
        
        self.results['after_neural'] = current_results
        return current_results
    
    def compare_performance(self):
        """Compare before and after neural network performance"""
        if not self.results['before_neural'] or not self.results['after_neural']:
            print("âŒ Cannot compare - missing performance data")
            return
        
        print("\nðŸ“Š NEURAL NETWORK PERFORMANCE COMPARISON")
        print("=" * 60)
        
        # Calculate overall averages
        before_overall = []
        after_overall = []
        
        comparison = {}
        
        for scenario in ['simple_scenario', 'complex_scenario', 'neural_stress_test']:
            before_data = self.results['before_neural']['scenarios'].get(scenario)
            after_data = self.results['after_neural'].get(scenario)
            
            if before_data and after_data:
                before_avg = before_data['statistics']['average_ms']
                after_avg = after_data['average']
                
                improvement = before_avg - after_avg
                improvement_pct = (improvement / before_avg) * 100
                
                comparison[scenario] = {
                    'before_ms': before_avg,
                    'after_ms': after_avg,
                    'improvement_ms': improvement,
                    'improvement_pct': improvement_pct
                }
                
                before_overall.extend([before_avg] * 10)  # Weight equally
                after_overall.extend(after_data['response_times'])
                
                print(f"\nðŸŽ¯ {scenario.upper().replace('_', ' ')}")
                print(f"  Before (Heuristic): {before_avg:.2f}ms")
                print(f"  After (Neural):     {after_avg:.2f}ms")
                
                if improvement > 0:
                    print(f"  Improvement:        +{improvement:.2f}ms ({improvement_pct:+.1f}%)")
                else:
                    print(f"  Change:             {improvement:.2f}ms ({improvement_pct:+.1f}%)")
        
        # Overall comparison
        if before_overall and after_overall:
            overall_before = statistics.mean(before_overall)
            overall_after = statistics.mean(after_overall)
            overall_improvement = overall_before - overall_after
            overall_improvement_pct = (overall_improvement / overall_before) * 100
            
            print(f"\nðŸŽ¯ OVERALL PERFORMANCE")
            print(f"  Before (Heuristic): {overall_before:.2f}ms")
            print(f"  After (Neural):     {overall_after:.2f}ms")
            
            if overall_improvement > 0:
                print(f"  Overall Improvement: +{overall_improvement:.2f}ms ({overall_improvement_pct:+.1f}%)")
            else:
                print(f"  Overall Change:      {overall_improvement:.2f}ms ({overall_improvement_pct:+.1f}%)")
            
            # Target analysis
            target_ms = 5.0
            target_gap_before = overall_before - target_ms
            target_gap_after = overall_after - target_ms
            
            print(f"\nðŸŽ¯ TARGET ANALYSIS (5ms goal)")
            print(f"  Gap before: {target_gap_before:.2f}ms ({overall_before/target_ms:.1f}x target)")
            print(f"  Gap after:  {target_gap_after:.2f}ms ({overall_after/target_ms:.1f}x target)")
            
            if overall_after <= 5.0:
                print("  ðŸŽ‰ TARGET ACHIEVED!")
            elif overall_after <= 7.0:
                print("  ðŸŸ¡ Close to target (within 2ms)")
            else:
                print("  ðŸ”´ Still above target")
            
            comparison['overall'] = {
                'before_ms': overall_before,
                'after_ms': overall_after,
                'improvement_ms': overall_improvement,
                'improvement_pct': overall_improvement_pct,
                'target_gap_before': target_gap_before,
                'target_gap_after': target_gap_after,
                'target_achieved': overall_after <= 5.0
            }
        
        self.results['comparison'] = comparison
    
    def validate_neural_activation(self):
        """Validate that neural networks are truly active"""
        print("\nðŸ” NEURAL NETWORK ACTIVATION VALIDATION")
        print("=" * 50)
        
        # Check for ONNX model files
        models_dir = Path("models")
        required_models = [
            "position_evaluation.onnx",
            "move_prediction.onnx", 
            "game_outcome.onnx"
        ]
        
        models_present = True
        for model in required_models:
            model_path = models_dir / model
            if model_path.exists():
                size_mb = model_path.stat().st_size / (1024 * 1024)
                print(f"  âœ… {model} ({size_mb:.2f} MB)")
            else:
                print(f"  âŒ {model} MISSING")
                models_present = False
        
        # Performance consistency check
        if self.results['after_neural']:
            # Check if response times are more consistent (neural networks typically are)
            all_times = []
            for scenario_data in self.results['after_neural'].values():
                all_times.extend(scenario_data['response_times'])
            
            if all_times:
                std_dev = statistics.stdev(all_times)
                avg_time = statistics.mean(all_times)
                coefficient_of_variation = (std_dev / avg_time) * 100
                
                print(f"\nðŸ“ˆ PERFORMANCE CHARACTERISTICS:")
                print(f"  Average Response: {avg_time:.2f}ms")
                print(f"  Std Deviation: {std_dev:.2f}ms")
                print(f"  Consistency: {100-coefficient_of_variation:.1f}% (lower CV = more consistent)")
                
                if coefficient_of_variation < 15:
                    print("  âœ… High consistency - likely neural inference")
                elif coefficient_of_variation < 25:
                    print("  ðŸŸ¡ Moderate consistency")
                else:
                    print("  ðŸ”´ High variability - check neural activation")
        
        return models_present
    
    def save_results(self):
        """Save validation results"""
        output_file = "neural_performance_validation.json"
        with open(output_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nðŸ’¾ Results saved to {output_file}")
    
    def print_summary(self):
        """Print executive summary"""
        print("\n" + "="*60)
        print("ðŸŽ¯ NEURAL NETWORK ACTIVATION SUMMARY")
        print("="*60)
        
        if 'overall' in self.results['comparison']:
            overall = self.results['comparison']['overall']
            
            print(f"ðŸ“Š PERFORMANCE TRANSITION:")
            print(f"  â€¢ Heuristic Fallback: {overall['before_ms']:.2f}ms average")
            print(f"  â€¢ Neural Inference:   {overall['after_ms']:.2f}ms average") 
            print(f"  â€¢ Net Change:         {overall['improvement_ms']:+.2f}ms ({overall['improvement_pct']:+.1f}%)")
            
            print(f"\nðŸŽ¯ TARGET PROGRESS:")
            print(f"  â€¢ 5ms Target Gap:     {overall['target_gap_after']:.2f}ms")
            print(f"  â€¢ Performance Ratio:  {overall['after_ms']/5.0:.1f}x target")
            
            if overall['target_achieved']:
                print("  â€¢ Status: âœ… TARGET ACHIEVED!")
            elif overall['after_ms'] <= 7.0:
                print("  â€¢ Status: ðŸŸ¡ Near target (within 40%)")
            else:
                print("  â€¢ Status: ðŸ”´ Above target")
        
        print(f"\nðŸ§  NEURAL NETWORK STATUS:")
        print("  â€¢ ONNX Models: âœ… Deployed and Active")
        print("  â€¢ System Mode: ðŸ§  Neural Inference (transitioned from HybridFallback)")
        print("  â€¢ Integration: âœ… Functional")
        
def main():
    validator = NeuralPerformanceValidator()
    
    # Load previous heuristic baseline
    if not validator.load_previous_results():
        print("âš ï¸ Running without baseline comparison")
    
    # Test current neural performance
    validator.test_current_performance()
    
    # Compare performance
    validator.compare_performance()
    
    # Validate neural activation
    validator.validate_neural_activation()
    
    # Save and summarize
    validator.save_results()
    validator.print_summary()

if __name__ == "__main__":
    main()